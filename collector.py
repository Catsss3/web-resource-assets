import os, sys, base64, json, logging, time, requests

# 1Ô∏è‚É£ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    filename="collector.log",
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
)

def log_info(msg: str) -> None:
    print(msg)
    logging.info(msg)

def log_error(msg: str) -> None:
    print(f"‚ùå {msg}")
    logging.error(msg)

# 2Ô∏è‚É£ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
WORKFLOW_TOKEN = os.getenv("WORKFLOW_TOKEN")
GITHUB_TOKEN   = os.getenv("GITHUB_TOKEN")
REPO           = os.getenv("GITHUB_REPOSITORY", "Catsss3/web-resource-assets")

TOKEN = WORKFLOW_TOKEN or GITHUB_TOKEN
if not TOKEN:
    log_error("–¢–æ–∫–µ–Ω WORKFLOW_TOKEN/GITHUB_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω")
    sys.exit(1)

HEADERS = {
    "Authorization": f"token {TOKEN}",
    "Accept": "application/vnd.github.v3+json",
    "User-Agent": "Blondie-Smart-Collector/2.0",
}

# 3Ô∏è‚É£ –†–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–æ–º sources.txt
def load_sources(path: str = "sources.txt") -> list[str]:
    if os.path.exists(path):
        with open(path, encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip()]
    url = f"https://api.github.com/repos/{REPO}/contents/{path}"
    resp = requests.get(url, headers=HEADERS, timeout=10)
    if resp.status_code == 200:
        content = base64.b64decode(resp.json()["content"]).decode()
        return [line.strip() for line in content.splitlines() if line.strip()]
    return []

def save_sources(sources: list[str], path: str = "sources.txt") -> None:
    content = "\n".join(sources) + "\n"
    push_file(path, content, "üì° Blondie: Update sources list")

# 4Ô∏è‚É£ –ü–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ GitHub
def discover_new_sources(existing: set[str]) -> set[str]:
    log_info("üîé –ü–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ GitHub...")
    new_found = set()
    search_url = "https://api.github.com/search/code"
    params = {
        "q": "vless:// in:file language:yaml",
        "per_page": 30,
        "page": 1,
    }
    try:
        resp = requests.get(search_url, headers=HEADERS, params=params, timeout=15)
        resp.raise_for_status()
        items = resp.json().get("items", [])
        for item in items:
            raw_url = (
                item["html_url"]
                .replace("github.com", "raw.githubusercontent.com")
                .replace("/blob/", "/")
            )
            if raw_url not in existing:
                new_found.add(raw_url)
    except Exception as e:
        log_error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
    return new_found

# 5Ô∏è‚É£ –í–∞–ª–∏–¥–∞—Ü–∏—è VLESS‚Äë—Å—Å—ã–ª–æ–∫
def is_valid_vless(link: str) -> bool:
    if not link.startswith("vless://"):
        return False
    low = link.lower()
    return ("security=tls" in low) or ("reality" in low)

# 6Ô∏è‚É£ –ó–∞–ø–∏—Å—å —Ñ–∞–π–ª–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
def push_file(path: str, content: str, msg: str) -> None:
    url = f"https://api.github.com/repos/{REPO}/contents/{path}"
    resp = requests.get(url, headers=HEADERS, timeout=10)
    sha = resp.json().get("sha") if resp.status_code == 200 else None
    data = {
        "message": msg,
        "content": base64.b64encode(content.encode()).decode(),
        "sha": sha,
    }
    put_resp = requests.put(url, headers=HEADERS, json=data, timeout=10)
    if put_resp.status_code in (200, 201):
        log_info(f"‚úÖ {msg}")
    else:
        log_error(f"‚ùå –û—à–∏–±–∫–∞ {put_resp.status_code}: {put_resp.text}")

# 7Ô∏è‚É£ –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å–±–æ—Ä–∞
def collect() -> None:
    current_sources = set(load_sources())
    log_info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(current_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    new_sources = discover_new_sources(current_sources)
    if new_sources:
        updated = sorted(current_sources.union(new_sources))
        save_sources(updated)
        current_sources = set(updated)
        log_info(f"üîé –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_sources)} –Ω–æ–≤—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    else:
        log_info("üîé –ù–æ–≤—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    found_links = []
    for url in current_sources:
        try:
            resp = requests.get(url, timeout=10)
            if resp.status_code == 200:
                for line in resp.text.splitlines():
                    line = line.strip()
                    if is_valid_vless(line):
                        found_links.append(line)
        except Exception as e:
            log_error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}")
    if not found_links:
        log_info("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –ø–æ–¥—Ö–æ–¥—è—â–µ–π —Å—Å—ã–ª–∫–∏")
        return
    unique_links = list(dict.fromkeys(found_links))[:100]
    final_content = "\n".join(unique_links) + "\n"
    push_file("input/fresh_raw_links.txt", final_content, "üì° Blondie: Daily Scrape Results")
    log_info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(unique_links)} —Å—Å—ã–ª–æ–∫")

if __name__ == "__main__":
    collect()
